{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28ce6a8c",
   "metadata": {},
   "source": [
    "## Benchmarking scVI MLflow Model with czbenchmarks\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "- Package a pre-trained scVI model using MLflow\n",
    "- Use czbenchmarks datasets for evaluation\n",
    "- Run inference through the MLflow model interface\n",
    "- Evaluate the model embeddings using czbenchmarks tasks\n",
    "- Compare results with PCA baselines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4665da0",
   "metadata": {},
   "source": [
    "\n",
    "### Setup and Model Packaging\n",
    "\n",
    "In this section, we'll clone the scvi_mlflow_pkg repository, set up the environment, and package the scVI model as an MLflow artifact. We'll modify the requirements to work on Mac by commenting out NVIDIA-specific libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Clone the repository if it doesn't exist\n",
    "repo_url = \"https://github.com/chanzuckerberg/vcp-model-pkg-client-tools.git\"\n",
    "repo_dir = \"vcp-model-pkg-client-tools\"\n",
    "\n",
    "if not os.path.exists(repo_dir):\n",
    "    print(\"üîÑ Cloning repository...\")\n",
    "    subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
    "    print(\"‚úÖ Repository cloned successfully\")\n",
    "\n",
    "# Navigate to the scvi_mlflow_pkg directory\n",
    "scvi_pkg_dir = os.path.join(repo_dir, \"examples\", \"mlflow_pkgs\", \"scvi_mlflow_pkg\")\n",
    "os.chdir(scvi_pkg_dir)\n",
    "\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Read and modify requirements.txt to remove NVIDIA dependencies for Mac compatibility\n",
    "requirements_path = \"requirements.txt\"\n",
    "if os.path.exists(requirements_path):\n",
    "    with open(requirements_path, 'r') as f:\n",
    "        requirements = f.read()\n",
    "    \n",
    "    # Comment out NVIDIA-related libraries\n",
    "    nvidia_libs = ['nvidia-', 'cupy-', 'torch+cu', 'cuda']\n",
    "    modified_requirements = []\n",
    "    \n",
    "    for line in requirements.split('\\n'):\n",
    "        if any(lib in line.lower() for lib in nvidia_libs):\n",
    "            modified_requirements.append(f\"# {line}  # Commented for Mac compatibility\")\n",
    "        else:\n",
    "            modified_requirements.append(line)\n",
    "    \n",
    "    # Write modified requirements\n",
    "    with open(requirements_path, 'w') as f:\n",
    "        f.write('\\n'.join(modified_requirements))\n",
    "    \n",
    "    print(\"‚úÖ Modified requirements.txt for Mac compatibility\")\n",
    "\n",
    "# Create virtual environment for MLflow packaging\n",
    "venv_name = \".venv_mlflow_scvi\"\n",
    "if not os.path.exists(venv_name):\n",
    "    print(f\"üîÑ Creating virtual environment: {venv_name}\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"venv\", venv_name], check=True)\n",
    "    print(\"‚úÖ Virtual environment created\")\n",
    "\n",
    "# Install requirements in the virtual environment\n",
    "pip_path = os.path.join(venv_name, \"bin\", \"pip\") if os.name != 'nt' else os.path.join(venv_name, \"Scripts\", \"pip.exe\")\n",
    "python_path = os.path.join(venv_name, \"bin\", \"python\") if os.name != 'nt' else os.path.join(venv_name, \"Scripts\", \"python.exe\")\n",
    "\n",
    "print(\"üîÑ Installing requirements...\")\n",
    "subprocess.run([pip_path, \"install\", \"--upgrade\", \"pip\"], check=True)\n",
    "subprocess.run([pip_path, \"install\", \"--no-deps\", \"-r\", requirements_path], check=True)\n",
    "print(\"‚úÖ Requirements installed\")\n",
    "\n",
    "# Download model artifacts from S3\n",
    "model_data_dir = \"model_data\"\n",
    "os.makedirs(model_data_dir, exist_ok=True)\n",
    "\n",
    "print(\"üîÑ Downloading model artifacts from S3...\")\n",
    "try:\n",
    "    # Download human model\n",
    "    subprocess.run([\n",
    "        \"aws\", \"s3\", \"sync\", \n",
    "        \"s3://cz-benchmarks-data/models/v1/scvi_2023_12_15/homo_sapiens\", \n",
    "        f\"{model_data_dir}/homo_sapiens\"\n",
    "    ], check=True)\n",
    "    \n",
    "    # Download mouse model\n",
    "    subprocess.run([\n",
    "        \"aws\", \"s3\", \"sync\", \n",
    "        \"s3://cz-benchmarks-data/models/v1/scvi_2023_12_15/mus_musculus\", \n",
    "        f\"{model_data_dir}/mus_musculus\"\n",
    "    ], check=True)\n",
    "    \n",
    "    print(\"‚úÖ Model artifacts downloaded successfully\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ùå Error downloading model artifacts: {e}\")\n",
    "    print(\"Please ensure AWS CLI is installed and configured with appropriate credentials\")\n",
    "\n",
    "# Package the MLflow model\n",
    "print(\"üîÑ Packaging MLflow model...\")\n",
    "mlflow_artifact_dir = \"mlflow_model_artifact\"\n",
    "\n",
    "# Remove existing artifact directory if it exists\n",
    "if os.path.exists(mlflow_artifact_dir):\n",
    "    shutil.rmtree(mlflow_artifact_dir)\n",
    "\n",
    "# Run the MLflow packager\n",
    "package_cmd = [\n",
    "    python_path, \"mlflow_packager.py\",\n",
    "    \"--model-class\", \"model_code.scvi_mlflow_model:ScviMLflowModel\",\n",
    "    \"--artifact\", \"homo_sapiens=homo_sapiens\",\n",
    "    \"--artifact\", \"mus_musculus=mus_musculus\", \n",
    "    \"--model-config-json\", '{\"organism\":\"human\"}',\n",
    "    \"--skip-inference\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    subprocess.run(package_cmd, check=True, cwd=\".\")\n",
    "    print(\"‚úÖ MLflow model packaged successfully\")\n",
    "    \n",
    "    # Verify the artifact structure\n",
    "    if os.path.exists(mlflow_artifact_dir):\n",
    "        print(f\"üì¶ MLflow model artifact created at: {os.path.abspath(mlflow_artifact_dir)}\")\n",
    "        print(\"üìÅ Artifact structure:\")\n",
    "        for root, dirs, files in os.walk(mlflow_artifact_dir):\n",
    "            level = root.replace(mlflow_artifact_dir, '').count(os.sep)\n",
    "            indent = ' ' * 2 * level\n",
    "            print(f\"{indent}{os.path.basename(root)}/\")\n",
    "            subindent = ' ' * 2 * (level + 1)\n",
    "            for file in files:\n",
    "                print(f\"{subindent}{file}\")\n",
    "    \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ùå Error packaging MLflow model: {e}\")\n",
    "\n",
    "# Store the MLflow model path for later use\n",
    "MLFLOW_MODEL_PATH = os.path.abspath(mlflow_artifact_dir)\n",
    "print(f\"üéØ MLflow model ready at: {MLFLOW_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558655c1",
   "metadata": {},
   "source": [
    "### Dataset Preparation with czbenchmarks\n",
    "\n",
    "Now we'll load a dataset from czbenchmarks and prepare it for inference with our MLflow-packaged scVI model. We need to ensure the dataset has the required observation columns (batch_keys) and save it in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fe680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change back to the original working directory for czbenchmarks\n",
    "os.chdir(\"../../../..\")  # Navigate back to the notebook directory\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Install czbenchmarks if not already installed\n",
    "try:\n",
    "    import czbenchmarks\n",
    "    print(\"‚úÖ czbenchmarks already available\")\n",
    "except ImportError:\n",
    "    print(\"üîÑ Installing czbenchmarks...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"czbenchmarks\"], check=True)\n",
    "    print(\"‚úÖ czbenchmarks installed\")\n",
    "\n",
    "# Import required libraries\n",
    "import logging\n",
    "import functools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from czbenchmarks.datasets import load_dataset\n",
    "from czbenchmarks.datasets.single_cell_labeled import SingleCellLabeledDataset\n",
    "import anndata as ad\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"--- Loading and Preparing czbenchmarks Dataset ---\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the dataset\n",
    "print(\"üîÑ Loading tsv2_prostate dataset...\")\n",
    "dataset: SingleCellLabeledDataset = load_dataset(\"tsv2_prostate\")\n",
    "print(\"‚úÖ Dataset loaded successfully\")\n",
    "print(f\"üìä Dataset shape: {dataset.adata.shape}\")\n",
    "print(f\"üè∑Ô∏è  Labels: {dataset.labels.name} with {len(dataset.labels.unique())} unique values\")\n",
    "\n",
    "# Prepare the dataset for scVI MLflow model\n",
    "print(\"\\nüîÑ Preparing dataset for scVI MLflow model...\")\n",
    "\n",
    "# The MLflow model expects specific batch_keys - let's check what we have\n",
    "adata = dataset.adata.copy()\n",
    "print(f\"üìã Available observation columns: {list(adata.obs.columns)}\")\n",
    "\n",
    "# Default batch_keys expected by the model\n",
    "expected_batch_keys = [\"dataset_id\", \"assay\", \"suspension_type\", \"donor_id\"]\n",
    "\n",
    "# Check which batch_keys are available and create missing ones\n",
    "available_batch_keys = []\n",
    "for key in expected_batch_keys:\n",
    "    if key in adata.obs.columns:\n",
    "        available_batch_keys.append(key)\n",
    "        print(f\"‚úÖ Found {key}: {adata.obs[key].nunique()} unique values\")\n",
    "    else:\n",
    "        # Create a default value for missing batch keys\n",
    "        adata.obs[key] = f\"default_{key}\"\n",
    "        available_batch_keys.append(key)\n",
    "        print(f\"‚ö†Ô∏è  Created missing {key} with default value\")\n",
    "\n",
    "# Create batch identifier by combining batch_keys\n",
    "print(f\"üîó Creating batch identifier from: {available_batch_keys}\")\n",
    "adata.obs[\"batch\"] = functools.reduce(\n",
    "    lambda a, b: a + b, \n",
    "    [adata.obs[c].astype(str) + \"_\" for c in available_batch_keys]\n",
    ")\n",
    "\n",
    "# Remove trailing underscore\n",
    "adata.obs[\"batch\"] = adata.obs[\"batch\"].str.rstrip(\"_\")\n",
    "\n",
    "print(f\"üì¶ Created {adata.obs['batch'].nunique()} unique batch identifiers\")\n",
    "\n",
    "# Save the prepared dataset\n",
    "prepared_data_path = \"prepared_dataset.h5ad\"\n",
    "adata.write_h5ad(prepared_data_path)\n",
    "print(f\"üíæ Prepared dataset saved to: {os.path.abspath(prepared_data_path)}\")\n",
    "\n",
    "# Store information for next steps\n",
    "DATASET_PATH = os.path.abspath(prepared_data_path)\n",
    "BATCH_KEYS_STR = \",\".join(available_batch_keys)\n",
    "ORIGINAL_LABELS = dataset.labels.copy()\n",
    "ORIGINAL_ADATA_FOR_BASELINE = dataset.adata.copy()  # Keep original for baseline computation\n",
    "\n",
    "print(f\"üéØ Dataset ready for MLflow model inference\")\n",
    "print(f\"üìÅ Dataset path: {DATASET_PATH}\")\n",
    "print(f\"üîë Batch keys: {BATCH_KEYS_STR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ada936",
   "metadata": {},
   "source": [
    "### MLflow Model Inference\n",
    "\n",
    "In this section, we'll prepare the input for our MLflow-packaged scVI model, run inference, and extract the embeddings. The MLflow model expects input in a specific JSON format with the dataset path and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5febee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"--- Running MLflow Model Inference ---\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare input for MLflow model\n",
    "print(\"üîÑ Preparing MLflow model input...\")\n",
    "\n",
    "# Create the input JSON structure expected by the MLflow model\n",
    "mlflow_input = {\n",
    "    \"dataframe_split\": {\n",
    "        \"columns\": [\"input_uri\"],\n",
    "        \"data\": [[DATASET_PATH]]\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"organism\": \"human\",\n",
    "        \"return_dist\": True,  # Return mean embeddings\n",
    "        \"batch_keys\": BATCH_KEYS_STR\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save input to temporary file\n",
    "input_json_path = \"mlflow_input.json\"\n",
    "with open(input_json_path, 'w') as f:\n",
    "    json.dump(mlflow_input, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ MLflow input prepared:\")\n",
    "print(json.dumps(mlflow_input, indent=2))\n",
    "\n",
    "# Run MLflow model prediction\n",
    "print(f\"\\nüîÑ Running inference with MLflow model...\")\n",
    "print(f\"üì¶ Model path: {MLFLOW_MODEL_PATH}\")\n",
    "print(f\"üìÑ Input path: {os.path.abspath(input_json_path)}\")\n",
    "\n",
    "output_json_path = \"mlflow_output.json\"\n",
    "\n",
    "try:\n",
    "    # Use MLflow's predict API\n",
    "    print(\"üöÄ Starting MLflow model prediction...\")\n",
    "    \n",
    "    # Load the MLflow model\n",
    "    loaded_model = mlflow.pyfunc.load_model(MLFLOW_MODEL_PATH)\n",
    "    \n",
    "    # Prepare input DataFrame for prediction\n",
    "    input_df = pd.DataFrame({\"input_uri\": [DATASET_PATH]})\n",
    "    \n",
    "    # Run prediction with parameters\n",
    "    predictions = loaded_model.predict(\n",
    "        input_df, \n",
    "        params={\n",
    "            \"organism\": \"human\",\n",
    "            \"return_dist\": True,\n",
    "            \"batch_keys\": BATCH_KEYS_STR\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ MLflow model prediction completed successfully\")\n",
    "    print(f\"üìä Predictions shape: {predictions.shape}\")\n",
    "    print(f\"üìà Predictions type: {type(predictions)}\")\n",
    "    \n",
    "    # Convert to numpy array if needed\n",
    "    if hasattr(predictions, 'values'):\n",
    "        model_embeddings = predictions.values\n",
    "    else:\n",
    "        model_embeddings = np.array(predictions)\n",
    "    \n",
    "    print(f\"üéØ Generated embeddings shape: {model_embeddings.shape}\")\n",
    "    print(f\"üìä Embedding statistics:\")\n",
    "    print(f\"   Mean: {model_embeddings.mean():.4f}\")\n",
    "    print(f\"   Std: {model_embeddings.std():.4f}\")\n",
    "    print(f\"   Min: {model_embeddings.min():.4f}\")\n",
    "    print(f\"   Max: {model_embeddings.max():.4f}\")\n",
    "    \n",
    "    # Save embeddings for later use\n",
    "    np.save(\"scvi_mlflow_embeddings.npy\", model_embeddings)\n",
    "    print(\"üíæ Embeddings saved to: scvi_mlflow_embeddings.npy\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running MLflow model: {e}\")\n",
    "    print(\"üîç Troubleshooting tips:\")\n",
    "    print(\"   1. Ensure the MLflow model was packaged correctly\")\n",
    "    print(\"   2. Check that the dataset has required batch_keys columns\")\n",
    "    print(\"   3. Verify the model artifacts are properly downloaded\")\n",
    "    raise\n",
    "\n",
    "# Verify embeddings are ready for czbenchmarks tasks\n",
    "print(f\"\\nüéØ Embeddings ready for czbenchmarks evaluation!\")\n",
    "print(f\"üìê Embedding dimensions: {model_embeddings.shape}\")\n",
    "print(f\"üî¢ Number of cells: {model_embeddings.shape[0]}\")\n",
    "print(f\"üìè Latent dimensions: {model_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb927dfc",
   "metadata": {},
   "source": [
    "### czbenchmarks Task Evaluation\n",
    "\n",
    "Finally, we'll use the embeddings from our MLflow model to run czbenchmarks tasks including clustering, embedding quality assessment, and metadata label prediction. We'll compare the results with PCA baselines and generate visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7592a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "import scanpy as sc\n",
    "\n",
    "# Import czbenchmarks tasks\n",
    "from czbenchmarks.tasks import (\n",
    "    ClusteringTask,\n",
    "    EmbeddingTask,\n",
    "    MetadataLabelPredictionTask,\n",
    ")\n",
    "from czbenchmarks.tasks.clustering import ClusteringTaskInput\n",
    "from czbenchmarks.tasks.embedding import EmbeddingTaskInput\n",
    "from czbenchmarks.tasks.label_prediction import MetadataLabelPredictionTaskInput\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"--- czbenchmarks Task Evaluation ---\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set up visualization style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.style.use('default')\n",
    "\n",
    "# Load embeddings from previous step\n",
    "try:\n",
    "    model_embeddings = np.load(\"scvi_mlflow_embeddings.npy\")\n",
    "    print(f\"‚úÖ Loaded MLflow model embeddings: {model_embeddings.shape}\")\n",
    "except:\n",
    "    print(\"‚ùå Could not load embeddings from previous step\")\n",
    "    raise\n",
    "\n",
    "# Prepare data for tasks\n",
    "expression_data = ORIGINAL_ADATA_FOR_BASELINE.X\n",
    "labels = ORIGINAL_LABELS\n",
    "obs_data = ORIGINAL_ADATA_FOR_BASELINE.obs\n",
    "\n",
    "print(f\"üìä Expression data shape: {expression_data.shape}\")\n",
    "print(f\"üè∑Ô∏è  Labels shape: {labels.shape}\")\n",
    "print(f\"üìã Number of unique labels: {labels.nunique()}\")\n",
    "\n",
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "# =============================================================================\n",
    "# Task 1: Clustering Performance\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"üéØ Task 1: Clustering Performance\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "print(\"üîÑ Initializing clustering task...\")\n",
    "clustering_task = ClusteringTask()\n",
    "clustering_task_input = ClusteringTaskInput(\n",
    "    obs=obs_data,\n",
    "    input_labels=labels,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Running clustering evaluation on MLflow model embeddings...\")\n",
    "clustering_results_model = clustering_task.run(\n",
    "    cell_representation=model_embeddings,\n",
    "    task_input=clustering_task_input,\n",
    ")\n",
    "\n",
    "print(\"üìä Computing PCA baseline...\")\n",
    "clustering_baseline_embedding = clustering_task.compute_baseline(expression_data)\n",
    "clustering_results_baseline = clustering_task.run(\n",
    "    cell_representation=clustering_baseline_embedding,\n",
    "    task_input=clustering_task_input,\n",
    ")\n",
    "\n",
    "# Store results\n",
    "all_results[\"clustering\"] = {\n",
    "    \"model\": [r.model_dump() for r in clustering_results_model],\n",
    "    \"baseline\": [r.model_dump() for r in clustering_results_baseline],\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Clustering evaluation completed\")\n",
    "\n",
    "# Visualize clustering results\n",
    "print(\"üìà Creating clustering performance visualization...\")\n",
    "df_clustering_model = pd.DataFrame(all_results[\"clustering\"][\"model\"])\n",
    "df_clustering_baseline = pd.DataFrame(all_results[\"clustering\"][\"baseline\"])\n",
    "df_clustering_model[\"source\"] = \"scVI MLflow Model\"\n",
    "df_clustering_baseline[\"source\"] = \"PCA Baseline\"\n",
    "df_clustering = pd.concat([df_clustering_model, df_clustering_baseline])\n",
    "df_clustering[\"metric_name\"] = df_clustering[\"metric_type\"].apply(lambda x: x.name)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=df_clustering, x=\"metric_name\", y=\"value\", hue=\"source\", \n",
    "    palette=[\"#2E86AB\", \"#A23B72\"]\n",
    ")\n",
    "plt.title(\"Clustering Performance: scVI MLflow Model vs. PCA Baseline\", fontsize=16, pad=20)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.xlabel(\"Metric\", fontsize=12)\n",
    "plt.legend(title=\"Method\", frameon=True)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"clustering_performance_mlflow.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# Task 2: Embedding Quality\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"üéØ Task 2: Embedding Quality Assessment\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "print(\"üîÑ Initializing embedding quality task...\")\n",
    "embedding_task = EmbeddingTask()\n",
    "embedding_task_input = EmbeddingTaskInput(input_labels=labels)\n",
    "\n",
    "print(\"üöÄ Evaluating MLflow model embedding quality...\")\n",
    "embedding_results_model = embedding_task.run(model_embeddings, embedding_task_input)\n",
    "\n",
    "print(\"üìä Computing PCA baseline...\")\n",
    "embedding_baseline_embedding = embedding_task.compute_baseline(expression_data)\n",
    "embedding_results_baseline = embedding_task.run(\n",
    "    embedding_baseline_embedding, embedding_task_input\n",
    ")\n",
    "\n",
    "# Store results\n",
    "all_results[\"embedding\"] = {\n",
    "    \"model\": [r.model_dump() for r in embedding_results_model],\n",
    "    \"baseline\": [r.model_dump() for r in embedding_results_baseline],\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Embedding quality evaluation completed\")\n",
    "\n",
    "# Visualize embedding quality results\n",
    "print(\"üìà Creating embedding quality visualization...\")\n",
    "df_embedding_model = pd.DataFrame(all_results[\"embedding\"][\"model\"])\n",
    "df_embedding_baseline = pd.DataFrame(all_results[\"embedding\"][\"baseline\"])\n",
    "df_embedding_model[\"source\"] = \"scVI MLflow Model\"\n",
    "df_embedding_baseline[\"source\"] = \"PCA Baseline\"\n",
    "df_embedding = pd.concat([df_embedding_model, df_embedding_baseline])\n",
    "df_embedding[\"metric_name\"] = df_embedding[\"metric_type\"].apply(lambda x: x.name)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=df_embedding, x=\"metric_name\", y=\"value\", hue=\"source\",\n",
    "    palette=[\"#F18F01\", \"#C73E1D\"]\n",
    ")\n",
    "plt.title(\"Embedding Quality: scVI MLflow Model vs. PCA Baseline\", fontsize=16, pad=20)\n",
    "plt.ylabel(\"Silhouette Score\", fontsize=12)\n",
    "plt.xlabel(\"Metric\", fontsize=12)\n",
    "plt.legend(title=\"Method\", frameon=True)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"embedding_quality_mlflow.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# Task 3: Metadata Label Prediction\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"üéØ Task 3: Metadata Label Prediction\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "print(\"üîÑ Initializing label prediction task...\")\n",
    "prediction_task = MetadataLabelPredictionTask()\n",
    "prediction_task_input = MetadataLabelPredictionTaskInput(labels=labels)\n",
    "\n",
    "print(\"üöÄ Running label prediction on MLflow model embeddings...\")\n",
    "prediction_results_model = prediction_task.run(model_embeddings, prediction_task_input)\n",
    "\n",
    "print(\"üìä Computing PCA baseline...\")\n",
    "prediction_baseline_embedding = prediction_task.compute_baseline(expression_data)\n",
    "prediction_results_baseline = prediction_task.run(\n",
    "    prediction_baseline_embedding, prediction_task_input\n",
    ")\n",
    "\n",
    "# Store results\n",
    "all_results[\"prediction\"] = {\n",
    "    \"model\": [r.model_dump() for r in prediction_results_model],\n",
    "    \"baseline\": [r.model_dump() for r in prediction_results_baseline],\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Label prediction evaluation completed\")\n",
    "\n",
    "# Visualize prediction results\n",
    "print(\"üìà Creating label prediction visualization...\")\n",
    "df_pred_model = pd.DataFrame(all_results[\"prediction\"][\"model\"])\n",
    "df_pred_baseline = pd.DataFrame(all_results[\"prediction\"][\"baseline\"])\n",
    "df_pred_model[\"source\"] = \"scVI MLflow Model\"\n",
    "df_pred_baseline[\"source\"] = \"PCA Baseline\"\n",
    "df_pred = pd.concat([df_pred_model, df_pred_baseline])\n",
    "df_pred[\"metric_name\"] = df_pred[\"metric_type\"].apply(lambda x: x.name)\n",
    "df_pred[\"classifier\"] = df_pred[\"params\"].apply(lambda p: p.get(\"classifier\", \"Overall\"))\n",
    "\n",
    "# Filter for mean metrics for cleaner visualization\n",
    "df_pred_mean = df_pred[df_pred[\"classifier\"].str.contains(\"MEAN\", na=False)]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(\n",
    "    data=df_pred_mean, x=\"metric_name\", y=\"value\", hue=\"source\",\n",
    "    palette=[\"#3A86FF\", \"#FF006E\"]\n",
    ")\n",
    "plt.title(\"Label Prediction Performance: scVI MLflow Model vs. PCA Baseline\", fontsize=16, pad=20)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.xlabel(\"Metric\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title=\"Method\", frameon=True)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"label_prediction_mlflow.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# Summary Results Table\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìä COMPREHENSIVE RESULTS SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "def create_summary_table(results_dict):\n",
    "    \"\"\"Create a comprehensive summary table of all results.\"\"\"\n",
    "    summary_rows = []\n",
    "    \n",
    "    for task, sources in results_dict.items():\n",
    "        for source, metrics in sources.items():\n",
    "            for metric in metrics:\n",
    "                metric_name = metric.get(\"metric_type\", {}).get(\"name\", \"Unknown\")\n",
    "                value = metric.get(\"value\", 0)\n",
    "                \n",
    "                # Format value based on metric type\n",
    "                if isinstance(value, float):\n",
    "                    formatted_value = f\"{value:.4f}\"\n",
    "                else:\n",
    "                    formatted_value = str(value)\n",
    "                \n",
    "                summary_rows.append({\n",
    "                    \"Task\": task.replace(\"_\", \" \").title(),\n",
    "                    \"Method\": \"scVI MLflow\" if source == \"model\" else \"PCA Baseline\",\n",
    "                    \"Metric\": metric_name,\n",
    "                    \"Score\": formatted_value\n",
    "                })\n",
    "    \n",
    "    return summary_rows\n",
    "\n",
    "# Create and display summary table\n",
    "summary_data = create_summary_table(all_results)\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\nüèÜ Performance Summary:\")\n",
    "print(tabulate(summary_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "# Calculate improvement over baseline\n",
    "print(f\"\\nüìà Performance Improvements (scVI MLflow vs PCA Baseline):\")\n",
    "improvement_summary = []\n",
    "\n",
    "for task in all_results:\n",
    "    model_results = pd.DataFrame(all_results[task][\"model\"])\n",
    "    baseline_results = pd.DataFrame(all_results[task][\"baseline\"])\n",
    "    \n",
    "    for _, model_row in model_results.iterrows():\n",
    "        metric_type = model_row[\"metric_type\"]\n",
    "        model_value = model_row[\"value\"]\n",
    "        \n",
    "        # Find corresponding baseline result\n",
    "        baseline_row = baseline_results[\n",
    "            baseline_results[\"metric_type\"].apply(lambda x: x == metric_type)\n",
    "        ]\n",
    "        \n",
    "        if not baseline_row.empty:\n",
    "            baseline_value = baseline_row.iloc[0][\"value\"]\n",
    "            improvement = ((model_value - baseline_value) / baseline_value) * 100\n",
    "            \n",
    "            improvement_summary.append({\n",
    "                \"Task\": task.replace(\"_\", \" \").title(),\n",
    "                \"Metric\": metric_type.name,\n",
    "                \"Improvement\": f\"{improvement:+.2f}%\"\n",
    "            })\n",
    "\n",
    "improvement_df = pd.DataFrame(improvement_summary)\n",
    "print(tabulate(improvement_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "print(f\"\\nüéØ Evaluation completed successfully!\")\n",
    "print(f\"üìÅ Visualizations saved:\")\n",
    "print(f\"   ‚Ä¢ clustering_performance_mlflow.png\")\n",
    "print(f\"   ‚Ä¢ embedding_quality_mlflow.png\") \n",
    "print(f\"   ‚Ä¢ label_prediction_mlflow.png\")\n",
    "\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "print(f\"   ‚Ä¢ scVI MLflow model generated {model_embeddings.shape[1]}D embeddings for {model_embeddings.shape[0]} cells\")\n",
    "print(f\"   ‚Ä¢ Performance compared against PCA baseline across multiple tasks\")\n",
    "print(f\"   ‚Ä¢ Results show the effectiveness of pre-trained scVI models via MLflow deployment\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
