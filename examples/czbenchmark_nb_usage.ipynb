{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02f1cbf",
   "metadata": {},
   "source": [
    "# Benchmarking with `czbenchmarks`\n",
    "\n",
    "`czbenchmarks` provides a standardized framework for benchmarking single-cell analysis models. This notebook demonstrates its core features for evaluating tasks such as embedding, clustering, and perturbation prediction.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Standardized Datasets:** _Preprocessed_, _benchmark-ready_ datasets.\n",
    "- **Comprehensive Metrics:** Validated evaluation metrics.\n",
    "- **Consistent Baselines:** Reference methods for comparison.\n",
    "- **Result Management:** Organized tracking of benchmarking results.\n",
    "\n",
    "## Components\n",
    "\n",
    "### Datasets\n",
    "\n",
    "Datasets are wrapped for consistent loading and compatibility:\n",
    "\n",
    "- `SingleCellLabeledDataset`: Gene expression data with cell labels (supports clustering, embedding, label prediction).\n",
    "- `SingleCellPerturbationDataset`: Perturbation datasets with control and perturbed cells.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "Each task defines an evaluation workflow with `run()` and `compute_baseline()` methods.\n",
    "\n",
    "| Task Name         | Class                         | Purpose                                         |\n",
    "|-------------------|-------------------------------|-------------------------------------------------|\n",
    "| Clustering        | `ClusteringTask`              | Evaluate cell group separation                  |\n",
    "| Embedding Quality | `EmbeddingTask`               | Assess embedding structure                      |\n",
    "| Label Prediction  | `MetadataLabelPredictionTask` | Predict labels from embeddings                  |\n",
    "| Batch Integration | `BatchIntegrationTask`        | Evaluate batch integration                      |\n",
    "| Cross-Species     | `CrossSpeciesIntegrationTask` | Integrate data across species                   |\n",
    "\n",
    "### Metrics\n",
    "\n",
    "Metrics are managed by `MetricRegistry` and returned as `MetricResult` objects.\n",
    "\n",
    "- `MetricType`: Enum of metric names (e.g., `ADJUSTED_RAND_INDEX`, `SILHOUETTE_SCORE`)\n",
    "- `MetricResult`: Stores metric type, value, and parameters\n",
    "\n",
    "All tasks compute and return metrics automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b9d9f4",
   "metadata": {},
   "source": [
    "## Example: Benchmarking an Embedding\n",
    "\n",
    "This section demonstrates how to benchmark a model's embedding using czbenchmarks. The workflow covers clustering, embedding quality, and label prediction tasks.\n",
    "\n",
    "### Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6678a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sgupta/cz-benchmarks/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from czbenchmarks.datasets import load_dataset\n",
    "from czbenchmarks.datasets.single_cell_labeled import SingleCellLabeledDataset\n",
    "from czbenchmarks.tasks.types import CellRepresentation\n",
    "from czbenchmarks.tasks import (\n",
    "    ClusteringTask,\n",
    "    EmbeddingTask,\n",
    "    MetadataLabelPredictionTask,\n",
    ")\n",
    "from czbenchmarks.tasks.clustering import ClusteringTaskInput\n",
    "from czbenchmarks.tasks.embedding import EmbeddingTaskInput\n",
    "from czbenchmarks.tasks.label_prediction import MetadataLabelPredictionTaskInput\n",
    "\n",
    "# Set up basic logging to see the library's output\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9bcfda",
   "metadata": {},
   "source": [
    "### Step 2: Load a Dataset\n",
    "\n",
    "Load the pre-configured `tsv2_prostate` dataset. The library handles automatic download, caching, and loading as a `SingleCellLabeledDataset` for streamlined reuse.\n",
    "\n",
    "**Loaded dataset provides:**\n",
    "- `dataset.adata`: AnnData object with gene expression data.\n",
    "- `dataset.labels`: pandas Series of cell type labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20bb6d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:czbenchmarks.file_utils:File already exists in cache: /Users/sgupta/.cz-benchmarks/datasets/homo_sapiens_10df7690-6d10-4029-a47e-0f071bb2df83_Prostate_v2_curated.h5ad\n",
      "INFO:czbenchmarks.datasets.single_cell:Loading dataset from /Users/sgupta/.cz-benchmarks/datasets/homo_sapiens_10df7690-6d10-4029-a47e-0f071bb2df83_Prostate_v2_curated.h5ad\n"
     ]
    }
   ],
   "source": [
    "# The 'dataset' object is a validated AnnData wrapper, ensuring efficient downstream processing.\n",
    "dataset: SingleCellLabeledDataset = load_dataset(\"tsv2_prostate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a14fb9",
   "metadata": {},
   "source": [
    "### Step 3: Prepare Model Output\n",
    "\n",
    "The library expects a **`CellRepresentation`** a `numpy.ndarray` with cells as rows and embedding features as columns. For demonstration, we simulate model output with random data.\n",
    "\n",
    "*Replace this with your model's actual embedding in practice.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b579ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated 10-dimensional embedding for each cell\n",
    "model_output: CellRepresentation = np.random.rand(dataset.adata.shape[0], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e4f49",
   "metadata": {},
   "source": [
    "### Step 4: Run the Clustering Task\n",
    "\n",
    "Evaluate the embedding by measuring clustering performance using Adjusted Rand Index (ARI) and Normalized Mutual Information (NMI). The task compares Leiden clusters from the embedding to true labels. Higher scores indicate better clustering. Compare `clustering_results` to `clustering_baseline_results` to assess model performance against the PCA baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77fb4ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Clustering Model Results ---\n",
      "{\n",
      "  \"metric_type\": \"adjusted_rand_index\",\n",
      "  \"value\": -0.0001785534951059197,\n",
      "  \"params\": {}\n",
      "}\n",
      "{\n",
      "  \"metric_type\": \"normalized_mutual_info\",\n",
      "  \"value\": 0.022872839500109668,\n",
      "  \"params\": {}\n",
      "}\n",
      "\n",
      "--- Clustering Baseline Results ---\n",
      "{\n",
      "  \"metric_type\": \"adjusted_rand_index\",\n",
      "  \"value\": 0.626707020983652,\n",
      "  \"params\": {}\n",
      "}\n",
      "{\n",
      "  \"metric_type\": \"normalized_mutual_info\",\n",
      "  \"value\": 0.8326481406592264,\n",
      "  \"params\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the task\n",
    "clustering_task = ClusteringTask()\n",
    "\n",
    "# 2. Define the inputs for the task\n",
    "clustering_task_input = ClusteringTaskInput(\n",
    "    obs=dataset.adata.obs,      # The full observation metadata\n",
    "    input_labels=dataset.labels # The ground-truth labels for comparison\n",
    ")\n",
    "\n",
    "# 3. Run the task on your model's output\n",
    "clustering_results = clustering_task.run(\n",
    "    cell_representation=model_output,\n",
    "    task_input=clustering_task_input,\n",
    ")\n",
    "\n",
    "# 4. Compute and run the baseline for comparison\n",
    "expression_data = dataset.adata.X\n",
    "clustering_baseline = clustering_task.compute_baseline(expression_data)\n",
    "clustering_baseline_results = clustering_task.run(\n",
    "    cell_representation=clustering_baseline,\n",
    "    task_input=clustering_task_input,\n",
    ")\n",
    "\n",
    "print(\"--- Clustering Model Results ---\")\n",
    "for result in clustering_results:\n",
    "    print(result.model_dump_json(indent=2))\n",
    "\n",
    "print(\"\\n--- Clustering Baseline Results ---\")\n",
    "for result in clustering_baseline_results:\n",
    "    print(result.model_dump_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
